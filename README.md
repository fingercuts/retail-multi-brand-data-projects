# ğŸ›’ Retail Multi-Brand Data Project

> â€œThis project simulates the analytics challenges of a multi-brand retail holding company.  
> From raw transactional extracts, I built an end-to-end data pipeline â†’ (ingestion â†’ transformation â†’ orchestration â†’ BI dashboard) to generate insights for business stakeholders.â€

**Project Highlights**
- Built full pipeline: raw data â†’ DuckDB/dbt marts â†’ BI dashboards.
- Tools: Google Colab, PySpark, DuckDB, dbt, Airflow, Power BI.
- Focus: Data engineering, analytics engineering, business intelligence, and storytelling.

---

## ğŸ“Œ Project Overview
This project simulates the challenges of working with a **large, messy multi-subsidiary retail dataset**.  
The goal is to showcase technical skills in:
- Data ingestion & cleaning (PySpark, DuckDB)
- Data modeling & transformations (dbt + DuckDB)
- Workflow orchestration (Airflow)
- Visualization & insights (Power BI)
- Business acumen & communication

Deliverables:
- GitHub repo (this one ğŸ–)
- Interactive Power BI Dashboard
- Analytics & insights presentation deck

---

## âš™ï¸ Tech Stack
- **Python (pandas, numpy, pyarrow)** â€“ preprocessing & Colab work
- **PySpark** â€“ ingestion & scalable cleaning
- **DuckDB** â€“ embedded analytical database
- **dbt** â€“ transformations, star schema modeling
- **Airflow** â€“ pipeline orchestration (simulated DAGs)
- **Power BI** â€“ dashboard visualization

---

## ğŸ—‚ï¸ Repository Structure

<pre>
data/
â”œâ”€â”€ raw/            # Raw CSV extracts (8 source files)
â”œâ”€â”€ staging/        # Cleaned Parquet files after ingestion
â””â”€â”€ marts/          # Analytics-ready tables (fact + dims from dbt)

notebooks/          # Colab notebooks (ingestion, EDA, analysis)
dbt_project/        # dbt models (staging, intermediate, marts)
airflow/            # Orchestration DAGs
dashboards/         # Power BI dashboards, exports
presentation/       # Insights deck
docs/               # Schema diagrams, lineage screenshots
</pre>

---

ğŸ”„ Data Pipeline Architecture
<pre> 
  Raw CSV extracts 
          â†“ 
  [Ingestion Layer] -> PySpark + DuckDB (staging tables, Parquet export) 
          â†“ 
  [Transformation] -> dbt + DuckDB (star schema: Fact + Dimensions, lineage docs) 
          â†“    
  [Orchestration] -> Airflow DAGs (simulate daily/weekly refresh) 
          â†“ 
  [Analytics Layer] -> Power BI dashboards (KPIs, trends, promos, store insights) 
          â†“ 
  [Business Output] -> Insights deck for executives 
</pre>

---

## ğŸ“ Star Schema
![Star Schema](docs/schema.png)

---

ğŸ“– [Data Dictionary](docs/data_dictionary.md)

---

## ğŸ› ï¸ Roadmap

1. **Ingestion** (CSV â†’ DuckDB + Parquet)  
2. **Data Modeling** (star schema design)  
3. **Transformation** (dbt models + lineage)  
4. **Orchestration** (Airflow DAG for pipeline flow)  
5. **Analytics & BI** (Power BI dashboards)  
6. **Insights & Storytelling** (presentation deck)  

---

## ğŸ“Š Visuals (Coming Soon)
- âœ… Data model (star schema diagram)  
- âœ… dbt lineage graph  
- âœ… Power BI dashboard screenshots  
- âœ… Final insights deck  

Stay tuned â€” updates as each phase completes! ğŸš€

---
